This repository serves as a comprehensive learning resource and a collection of code templates for training Large Language Models (LLMs). It is designed with user-friendly principles to help build efficient and reusable LLM training pipelines.

The content progresses step-by-step:

1. Basic Training: Starting with foundational LLM training using pure PyTorch.
2. Advanced Frameworks: Exploring high-level APIs like Hugging Face's transformers, DeepSpeed, and Fully Sharded Data Parallel (FSDP).
3. Comprehensive Tutorials: Concluding with simple, practical guides on leveraging cutting-edge open-source libraries, such as LLaMA-Factory and OpenRLHF, for complete LLM training workflows.

# TODO List
- [x] Supervised Fine-Tuning (SFT)
- [ ] Reinforcement Learning with Human Feedback (RLHF)
- [ ] Add additional sections (e.g., evaluation, deployment, etc.)


