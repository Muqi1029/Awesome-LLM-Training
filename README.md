# Awesome LLM Training Serving

This repository serves as a comprehensive learning resource and a collection of code templates for training and serving Large Language Models (LLMs). It is designed with user-friendly principles to get started with LLM training and help build efficient and reusable LLM training and serving pipelines.

## Training

### Distributed Training

Megatron-LM Training Framework

| Name | Year | Description |
| --- | --- | --- |
| [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](https://arxiv.org/abs/1811.06965) | 2018| Pipeline Parallelism |
| [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](<https://arxiv.org/pdf/1909.08053>) | 2020 | Distributed Training Framework |
| [MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core](https://www.arxiv.org/pdf/2504.14960) | 2025 | MoE Training |

## Serving

| Name | Year | Description |
| --- | --- | --- |
| [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135) | 2022 | Online Softmax |
| [FlashAttention2: One Write, Two Reads](https://arxiv.org/abs/2405.11299) | 2024 | Online Softmax |
