This repository serves as a comprehensive learning resource and a collection of code templates for training Large Language Models (LLMs). It is designed with user-friendly principles to help build efficient and reusable LLM training pipelines.

The content progresses step-by-step:

1. Basic Training: Starting with foundational LLM training using pure PyTorch.
2. Advanced Frameworks: Exploring high-level APIs like Hugging Face's transformers, DeepSpeed, and Fully Sharded Data Parallel (FSDP).
3. Comprehensive Tutorials: Concluding with simple, practical guides on leveraging cutting-edge open-source libraries, such as LLaMA-Factory and OpenRLHF, for complete LLM training workflows.

# TODO List
- [ ] Basic Data Parallel Training
- [ ] Basic Model Parallel Training
- [ ] Basic Pipeline Parallel Training
- [ ] Basic ZeRO Optimization
- [ ] Basic Distributed Training
- [ ] Basic Mixed Precision Training
- [ ] Basic Gradient Accumulation
- [ ] Supervised Fine-Tuning (SFT)
- [ ] Reinforcement Learning with Human Feedback (RLHF)
- [ ] Add additional sections (e.g., evaluation, deployment, etc.)
