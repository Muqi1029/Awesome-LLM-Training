# model
model_name_or_path: Qwen/Qwen2.5-7B-Instruct

# dataset
dataset: tatsu-lab/alpaca
max_length: 1024
data_loader_workers: 4
drop_last: false
shuffle: false
num_proc: 4
max_samples: 100

# training
grad_norm_clip: 1.0
output_dir: output/qwen_7b_alpaca
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 32
max_epochs: 3
save_steps: 200
save_total_limit: 2
learning_rate: 2e-5
logging_dir: logs/qwen_7b_alpaca

# deepspeed
deepspeed: config/deepspeed/zero_stage3_offload_config.json

# logging
run_name: qwen_7b_alpaca
wandb_project: qwen_7b_alpaca
logging_steps: 10
